Classifiers TODO:

--- TRADITIONAL ---

1. Logistic Regression: Despite being traditionally used for binary classification, it can be adapted for multi-class problems using techniques like one-vs-all or one-vs-one.

2. Gaussian Naive Bayes Classifier (GNB): This is a variant of the Naive Bayes classifier that assumes that the likelihood of the features is Gaussian. This makes it suitable for continuous data. Despite its simplicity and the strong assumption of independence between features, it can perform well in many cases, especially with high-dimensional data.

3. Quadratic Discriminant Analysis (QDA): This is a classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayesâ€™ rule. It provides a good compromise between the simplicity of Linear Discriminant Analysis (LDA) and the flexibility of a Naive Bayes classifier. It works well with a dataset in which each class is Gaussian distributed.

4. K-Nearest Neighbors (KNN): A simple and intuitive classifier that naturally handles multi-class problems.

5. Decision Trees: They naturally handle multi-class classification and provide interpretability.

6. Random Forest: An ensemble method using multiple decision trees, providing robustness and better accuracy.

7. Support Vector Machines (SVM): With the use of the 'one-vs-one' or 'one-vs-all' strategies, SVMs can be extended to handle multiple classes.

--- NON-TRADITIONAL --- 

8. Neural Networks: Powerful classifiers that can learn complex multi-class patterns, but may require more computational resources and time to train.

9. Deep Learning: This is a subset of machine learning that's based on artificial neural networks with representation learning. It can model complex patterns in data by using multiple processing layers with hierarchical representations, making it particularly effective for large and complex datasets. Deep learning techniques are behind many of the advancements in AI, including image and speech recognition, natural language processing, and more.

Computational Costs TODO:
- talk about the different computational costs related for different methods