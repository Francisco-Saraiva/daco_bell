{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9773818133999606\n",
      "Precision: 0.9773401825089539\n",
      "Recall: 0.9773818133999606\n",
      "F1 Score: 0.9773343861990973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming that your data is ready and is in the variables X_train, X_test, y_train, y_test\n",
    "# Load data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') - 1 # subtract 1 to make the labels 0, 1, 2 \n",
    "y_test = np.load('y_test.npy') -1    # same thing here\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # amount of data to be loaded each time\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim import Adam, SGD, RMSprop, Adagrad, Adamax, Adadelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') - 1 # subtract 1 to make the labels 0, 1, 2 \n",
    "y_test = np.load('y_test.npy') -1    # same thing here\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_epochs': [50, 100, 200, 300],\n",
    "    'optimizer': [Adam, SGD, RMSprop, Adagrad, Adamax, Adadelta]\n",
    "}\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Create a grid of hyperparameters\n",
    "grid = list(ParameterGrid(hyperparameters))\n",
    "\n",
    "# Convert the grid to a list and shuffle it to randomize the order of combinations\n",
    "random.shuffle(grid)\n",
    "\n",
    "# Use the first 60 combinations (make sure there are no duplicates)\n",
    "sampler = grid[:60]\n",
    "\n",
    "best_f1 = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for i, params in enumerate(sampler, start=1):\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a new model with the current hyperparameters\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 3),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "    # Define the loss and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Create DataLoaders with the current batch size\n",
    "    train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Print the number of models trained so far\n",
    "    print(f\"Trained {i} out of 60 models.\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Save the results\n",
    "    results.append({\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'optimizer': params['optimizer'].__name__,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time\n",
    "    })\n",
    "\n",
    "    # If the current model is better than all previous models, store its F1 score and hyperparameters\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_hyperparameters = params\n",
    "\n",
    "# Print the best accuracy and hyperparameters\n",
    "print('Best F1: {}'.format(best_f1))\n",
    "print('Best Hyperparameters: {}'.format(best_hyperparameters))\n",
    "\n",
    "# Convert the results to a dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "# Save the dataframe to a csv file\n",
    "results_df.to_csv('Results_Neural_Networks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
