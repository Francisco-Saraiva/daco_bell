{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9773818133999606\n",
      "Precision: 0.9773401825089539\n",
      "Recall: 0.9773818133999606\n",
      "F1 Score: 0.9773343861990973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming that your data is ready and is in the variables X_train, X_test, y_train, y_test\n",
    "# Load data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') - 1 # subtract 1 to make the labels 0, 1, 2 \n",
    "y_test = np.load('y_test.npy') -1    # same thing here\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # amount of data to be loaded each time\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 1 out of 60 models.\n",
      "Trained 2 out of 60 models.\n",
      "Trained 3 out of 60 models.\n",
      "Trained 4 out of 60 models.\n",
      "Trained 5 out of 60 models.\n",
      "Trained 6 out of 60 models.\n",
      "Trained 7 out of 60 models.\n",
      "Trained 8 out of 60 models.\n",
      "Trained 9 out of 60 models.\n",
      "Trained 10 out of 60 models.\n",
      "Trained 11 out of 60 models.\n",
      "Trained 12 out of 60 models.\n",
      "Trained 13 out of 60 models.\n",
      "Trained 14 out of 60 models.\n",
      "Trained 15 out of 60 models.\n",
      "Trained 16 out of 60 models.\n",
      "Trained 17 out of 60 models.\n",
      "Trained 18 out of 60 models.\n",
      "Trained 19 out of 60 models.\n",
      "Trained 20 out of 60 models.\n",
      "Trained 21 out of 60 models.\n",
      "Trained 22 out of 60 models.\n",
      "Trained 23 out of 60 models.\n",
      "Trained 24 out of 60 models.\n",
      "Trained 25 out of 60 models.\n",
      "Trained 26 out of 60 models.\n",
      "Trained 27 out of 60 models.\n",
      "Trained 28 out of 60 models.\n",
      "Trained 29 out of 60 models.\n",
      "Trained 30 out of 60 models.\n",
      "Trained 31 out of 60 models.\n",
      "Trained 32 out of 60 models.\n",
      "Trained 33 out of 60 models.\n",
      "Trained 34 out of 60 models.\n",
      "Trained 35 out of 60 models.\n",
      "Trained 36 out of 60 models.\n",
      "Trained 37 out of 60 models.\n",
      "Trained 38 out of 60 models.\n",
      "Trained 39 out of 60 models.\n",
      "Trained 40 out of 60 models.\n",
      "Trained 41 out of 60 models.\n",
      "Trained 42 out of 60 models.\n",
      "Trained 43 out of 60 models.\n",
      "Trained 44 out of 60 models.\n",
      "Trained 45 out of 60 models.\n",
      "Trained 46 out of 60 models.\n",
      "Trained 47 out of 60 models.\n",
      "Trained 48 out of 60 models.\n",
      "Trained 49 out of 60 models.\n",
      "Trained 50 out of 60 models.\n",
      "Trained 51 out of 60 models.\n",
      "Trained 52 out of 60 models.\n",
      "Trained 53 out of 60 models.\n",
      "Trained 54 out of 60 models.\n",
      "Trained 55 out of 60 models.\n",
      "Trained 56 out of 60 models.\n",
      "Trained 57 out of 60 models.\n",
      "Trained 58 out of 60 models.\n",
      "Trained 59 out of 60 models.\n",
      "Trained 60 out of 60 models.\n",
      "Best F1: 0.9792288338851821\n",
      "Best Hyperparameters: {'batch_size': 16, 'learning_rate': 0.1, 'num_epochs': 300, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim import Adam, SGD, RMSprop, Adagrad, Adamax, Adadelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Load data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') - 1 # subtract 1 to make the labels 0, 1, 2 \n",
    "y_test = np.load('y_test.npy') -1    # same thing here\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_epochs': [50, 100, 200, 300],\n",
    "    'optimizer': [Adam, SGD, RMSprop, Adagrad, Adamax, Adadelta]\n",
    "}\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Create a grid of hyperparameters\n",
    "grid = list(ParameterGrid(hyperparameters))\n",
    "\n",
    "# Convert the grid to a list and shuffle it to randomize the order of combinations\n",
    "random.shuffle(grid)\n",
    "\n",
    "# Use the first 60 combinations (make sure there are no duplicates)\n",
    "sampler = grid[:60]\n",
    "\n",
    "best_f1 = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for i, params in enumerate(sampler, start=1):\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a new model with the current hyperparameters\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 3),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "    # Define the loss and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Create DataLoaders with the current batch size\n",
    "    train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Print the number of models trained so far\n",
    "    print(f\"Trained {i} out of 60 models.\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Save the results\n",
    "    results.append({\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'optimizer': params['optimizer'].__name__,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time\n",
    "    })\n",
    "\n",
    "    # If the current model is better than all previous models, store its F1 score and hyperparameters\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_hyperparameters = params\n",
    "\n",
    "# Print the best accuracy and hyperparameters\n",
    "print('Best F1: {}'.format(best_f1))\n",
    "print('Best Hyperparameters: {}'.format(best_hyperparameters))\n",
    "\n",
    "# Convert the results to a dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "# Save the dataframe to a csv file\n",
    "results_df.to_csv('Results_Neural_Networks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  batch_size  num_epochs optimizer  accuracy  precision  \\\n",
      "0           0.10          16          50      Adam      0.85       0.86   \n",
      "1           0.01          32         100       SGD      0.89       0.90   \n",
      "\n",
      "   recall    f1  training_time  \n",
      "0    0.87  0.88            0.5  \n",
      "1    0.91  0.92            0.6  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "##! JUST A TEST FOR APPEND\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Append dictionaries to the list\n",
    "results.append({\n",
    "    'learning_rate': 0.1,\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 50,\n",
    "    'optimizer': 'Adam',\n",
    "    'accuracy': 0.85,\n",
    "    'precision': 0.86,\n",
    "    'recall': 0.87,\n",
    "    'f1': 0.88,\n",
    "    'training_time': 0.5\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 100,\n",
    "    'optimizer': 'SGD',\n",
    "    'accuracy': 0.89,\n",
    "    'precision': 0.90,\n",
    "    'recall': 0.91,\n",
    "    'f1': 0.92,\n",
    "    'training_time': 0.6\n",
    "})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
