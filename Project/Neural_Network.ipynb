{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9773818133999606\n",
      "Precision: 0.9773401825089539\n",
      "Recall: 0.9773818133999606\n",
      "F1 Score: 0.9773343861990973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming that your data is ready and is in the variables X_train, X_test, y_train, y_test\n",
    "# Load data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') - 1 # subtract 1 to make the labels 0, 1, 2 \n",
    "y_test = np.load('y_test.npy') -1    # same thing here\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # amount of data to be loaded each time\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_epochs': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Create a random sampler of hyperparameters\n",
    "sampler = ParameterSampler(hyperparameters, n_iter=10, random_state=0)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for params in sampler:\n",
    "    # Create a new model with the current hyperparameters\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 3),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "    # Define the loss and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Create DataLoaders with the current batch size\n",
    "    train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    # If the current model is better than all previous models, store its accuracy and hyperparameters\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hyperparameters = params\n",
    "\n",
    "# Print the best accuracy and hyperparameters\n",
    "print('Best Accuracy: {}'.format(best_accuracy))\n",
    "print('Best Hyperparameters: {}'.format(best_hyperparameters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
