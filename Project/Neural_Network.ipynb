{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9773818133999606\n",
      "Precision: 0.9773401825089539\n",
      "Recall: 0.9773818133999606\n",
      "F1 Score: 0.9773343861990973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming that your data is ready and is in the variables X_train, X_test, y_train, y_test\n",
    "# Load data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') - 1 # subtract 1 to make the labels 0, 1, 2 \n",
    "y_test = np.load('y_test.npy') -1    # same thing here\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # amount of data to be loaded each time\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 of 200 took 15.751048803329468 seconds.\n",
      "Epoch 50 of 50 took 4.316076278686523 seconds.\n",
      "Epoch 200 of 200 took 10.084126949310303 seconds.\n",
      "Epoch 200 of 200 took 7.290465831756592 seconds.\n",
      "Epoch 200 of 200 took 33.950913429260254 seconds.\n",
      "Epoch 200 of 200 took 10.952115535736084 seconds.\n",
      "Epoch 200 of 200 took 8.020126342773438 seconds.\n",
      "Epoch 100 of 100 took 11.8578519821167 seconds.\n",
      "Epoch 100 of 100 took 12.801377534866333 seconds.\n",
      "Epoch 200 of 200 took 5.056370735168457 seconds.\n",
      "Training took 5.056370735168457 seconds.\n",
      "Best F1: 0.9787889861588271\n",
      "Best Hyperparameters: {'num_epochs': 200, 'learning_rate': 0.001, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from torch.optim import Adam, SGD, RMSprop, Adagrad, Adamax, Adadelta\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_epochs': [100, 200, 300, 400, 500],\n",
    "    'optimizer': [Adam, SGD, RMSprop, Adagrad, Adamax, Adadelta]\n",
    "}\n",
    "\n",
    "# Create the dataframe to store the results\n",
    "results_df = pd.DataFrame(columns=['learning_rate', 'batch_size', 'num_epochs', \n",
    "                                   'optimizer', 'accuracy', 'precision', 'recall', 'f1', 'training_time'])\n",
    "\n",
    "# Create a random sampler of hyperparameters\n",
    "sampler = ParameterSampler(hyperparameters, n_iter=50, random_state=42)\n",
    "\n",
    "best_f1 = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Iterate over each combination of hyperparameters\n",
    "for i, params in enumerate(sampler, start=1):\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a new model with the current hyperparameters\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 3),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "    # Define the loss and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Create DataLoaders with the current batch size\n",
    "    train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Print the number of models trained so far\n",
    "    print(f\"Trained {i} out of 50 models.\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Save the results\n",
    "    results_df = results_df.append({\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'num_epochs': params['num_epochs'],\n",
    "        'optimizer': params['optimizer'].__name__,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # If the current model is better than all previous models, store its F1 score and hyperparameters\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_hyperparameters = params\n",
    "\n",
    "# Print the best accuracy and hyperparameters\n",
    "print('Best F1: {}'.format(best_f1))\n",
    "print('Best Hyperparameters: {}'.format(best_hyperparameters))\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "results_df.to_csv('Results_Neural_Networks.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
