{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # to save the trained models\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import all the scikit learn algorithms we used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Imports for Neural Networks\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# And for Convolutional Neural Networks\n",
    "from torch import flatten\n",
    "from torch import unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Convert the labels from 0 to 2, since some models need it, namely:\n",
    "# XGBoost, Neural Networks, Convolutional Neural Networks\n",
    "y_train_1 = y_train - 1\n",
    "y_test_1 = y_test - 1\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGALAXY (1)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQSO (2)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTAR (3)\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create a heatmap with the labels\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBlues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[0;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\matrix.py:110\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m--> 110\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[0;32m    113\u001b[0m mask \u001b[38;5;241m=\u001b[39m _matrix_mask(data, mask)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    771\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    779\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    780\u001b[0m         )\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 782\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:314\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    308\u001b[0m     _copy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    309\u001b[0m         copy_on_sanitize\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, dtype))\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     )\n\u001b[0;32m    313\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, copy\u001b[38;5;241m=\u001b[39m_copy)\n\u001b[1;32m--> 314\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\construction.py:592\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    590\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=()"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000, penalty='elasticnet', l1_ratio=0.0)\n",
    "\n",
    "time_start = time.time()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(logreg_model, open(\"Models/logistic_regression.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Logistic Regression', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m results\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGaussian Naive Bayes\u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy, precision, recall, f1, total_time])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGALAXY (1)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQSO (2)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTAR (3)\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create a heatmap with the labels\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "gaussianNB = GaussianNB()\n",
    "\n",
    "time_start = time.time()\n",
    "gaussianNB.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(gaussianNB, open(\"Models/gaussian_naive.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = gaussianNB.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Gaussian Naive Bayes', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "time_start = time.time()\n",
    "lda_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(lda_model, open(\"Models/lda.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = lda_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Linear Discriminant Analysis', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis(reg_param=0.0)\n",
    "\n",
    "time_start = time.time()\n",
    "qda_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(qda_model, open(\"Models/qda.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = qda_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Quadratic Discriminant Analysis', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(n_neighbors = 14, metric='minkowski', weights='uniform', algorithm='brute', leaf_size=33, p=2)\n",
    "\n",
    "time_start = time.time()\n",
    "KNN_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(KNN_model, open(\"Models/knn.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = KNN_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['K Nearest Neighbours', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(random_state=42, criterion='log_loss', max_depth=10, min_samples_leaf=10, min_samples_split=10)\n",
    "\n",
    "time_start = time.time()\n",
    "DT_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(DT_model, open(\"Models/decision_tree.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = DT_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Decision Tree', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestClassifier(n_estimators=25, random_state=42, max_depth=10, min_samples_leaf=5, criterion='entropy', oob_score=True) \n",
    "\n",
    "time_start = time.time()\n",
    "RF_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(RF_model, open(\"Models/random_forest.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = RF_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Random Forest', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 10, 'learning_rate': 0.1, 'objective': 'multi:softprob', 'num_class': 3, 'n_estimators' : 500}\n",
    "\n",
    "# Create the DMatrix from X_train and y_train_1\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_1)\n",
    "\n",
    "time_start = time.time()\n",
    "# Create the model and train\n",
    "xgb_model = xgb.train(params, dtrain)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(xgb_model, open(\"Models/xgboost.pkl\", \"wb\"))\n",
    "\n",
    "# Create a DMatrix from the test data\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Get the class with the highest probability for each sample\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test_1, y_pred)\n",
    "precision = precision_score(y_test_1, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_1, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_1, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['XGBoost', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test_1, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (by F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model_f1 = SVC(kernel='poly', random_state=42, decision_function_shape='ovr', gamma='scale', coef0=5.287036, degree=4, max_iter=1000000)\n",
    "\n",
    "time_start = time.time()\n",
    "SVC_model_f1.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(SVC_model_f1, open(\"Models/svm_f1.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = SVC_model_f1.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['SVM_by_f1', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (by Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model_time = SVC(kernel='poly', random_state=42, decision_function_shape='ovo', gamma='auto', coef0=8.523257, degree=2, max_iter=1000000)\n",
    "\n",
    "time_start = time.time()\n",
    "SVC_model_time.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(SVC_model_time, open(\"Models/svm_time.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = SVC_model_time.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['SVM_by_time', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network (by F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Hyperparameters = {'batch_size': 16, 'learning_rate': 0.1, 'num_epochs': 300, 'optimizer': 'Adagrad'}\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "batch_size = Best_Hyperparameters['batch_size']\n",
    "learning_rate = Best_Hyperparameters['learning_rate']\n",
    "epochs = Best_Hyperparameters['num_epochs']\n",
    "optimizer_name = Best_Hyperparameters['optimizer']\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_1, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders with the current batch size\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "NN_model_f1 = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "if optimizer_name == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(NN_model_f1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = NN_model_f1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print the progress\n",
    "    if epoch % 10 == 9:\n",
    "        print('Epoch: {}/{}...'.format(epoch+1, epochs), 'Loss: {:.6f}'.format(running_loss/len(train_loader)))\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(NN_model_f1.state_dict(), 'Models/NN_f1.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "NN_model_f1.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = NN_model_f1(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "total_time = end_time-start_time\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['NN_by_f1', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(all_labels, all_predictions)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network (by time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Hyperparameters = {'batch_size': 64, 'learning_rate': 0.0001, 'num_epochs': 50, 'optimizer': 'RMSprop'}\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "batch_size = Best_Hyperparameters['batch_size']\n",
    "learning_rate = Best_Hyperparameters['learning_rate']\n",
    "epochs = Best_Hyperparameters['num_epochs']\n",
    "optimizer_name = Best_Hyperparameters['optimizer']\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_1, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders with the current batch size\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "NN_model_time = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "if optimizer_name == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(NN_model_time.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = NN_model_time(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(NN_model_time.state_dict(), 'Models/NN_time.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "NN_model_time.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = NN_model_time(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "total_time = end_time-start_time\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['NN_by_time', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(all_labels, all_predictions)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (by F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Hyperparameters = {'batch_size': 32, 'learning_rate': 0.001, 'num_epochs': 100, 'optimizer': 'Adam'}\n",
    "\n",
    "# Get hyperparameters\n",
    "batch_size = Best_Hyperparameters['batch_size']\n",
    "learning_rate = Best_Hyperparameters['learning_rate']\n",
    "num_epochs = Best_Hyperparameters['num_epochs']\n",
    "optimizer_name = Best_Hyperparameters['optimizer']\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=20, kernel_size=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride = 2)\n",
    "\n",
    "        # Second layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=20, out_channels=40, kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride = 2)\n",
    "\n",
    "        # Third layer (fully connected)\n",
    "        self.fc1 = nn.Linear(in_features=40, out_features=20)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Softmax Classifier\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    # specifying the flow of data through the layers\n",
    "    def forward(self, x):\n",
    "        # Pass through conv1\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        # Pass through conv2\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # Flatten the output before fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        output = self.softmax(x)\n",
    "\n",
    "        return output  # Return the output\n",
    "    \n",
    "\n",
    "# Assuming X_train and X_test have shapes (num_samples, num_features)\n",
    "X_train_tensor = torch.tensor(X_train.transpose(0, 1), dtype=torch.float32).unsqueeze(1)  # Shape: (num_features, num_samples) -> (num_samples, 1, num_features)\n",
    "X_test_tensor = torch.tensor(X_test.transpose(0, 1), dtype=torch.float32).unsqueeze(1)    # Shape: (num_features, num_samples) -> (num_samples, 1, num_features)\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_1, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # amount of data to be loaded each time\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the more complex CNN\n",
    "CNN_f1 = Classifier(0.67)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if optimizer_name == 'Adam':\n",
    "    optimizer = optim.Adam(CNN_f1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = CNN_f1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(CNN_f1.state_dict(), 'Models/NN_time.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "CNN_f1.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = CNN_f1(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "total_time = end_time-start_time\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['CNN', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(all_labels, all_predictions)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (by time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi ^_^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Total Time'])\n",
    "results_df.to_csv('Model_Training_Results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
