{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # to save the trained models\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import all the scikit learn algorithms we used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Imports for Neural Networks\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# And for Convolutional Neural Networks\n",
    "from torch import flatten\n",
    "from torch import unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Convert the labels from 0 to 2, since some models need it, namely:\n",
    "# XGBoost, Neural Networks, Convolutional Neural Networks\n",
    "y_train_1 = y_train - 1\n",
    "y_test_1 = y_test - 1\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000, penalty='elasticnet', l1_ratio=0.0)\n",
    "\n",
    "time_start = time.time()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(logreg_model, open(\"Models/logistic_regression.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Logistic Regression', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussianNB = GaussianNB()\n",
    "\n",
    "time_start = time.time()\n",
    "gaussianNB.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(gaussianNB, open(\"Models/gaussian_naive.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = gaussianNB.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Gaussian Naive Bayes', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "time_start = time.time()\n",
    "lda_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(lda_model, open(\"Models/lda.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = lda_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Linear Discriminant Analysis', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis(reg_param=0.0)\n",
    "\n",
    "time_start = time.time()\n",
    "qda_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(qda_model, open(\"Models/qda.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = qda_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Quadratic Discriminant Analysis', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(n_neighbors = 14, metric='minkowski', weights='uniform', algorithm='brute', leaf_size=33, p=2)\n",
    "\n",
    "time_start = time.time()\n",
    "KNN_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(KNN_model, open(\"Models/knn.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = KNN_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['K Nearest Neighbours', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(random_state=42, criterion='log_loss', max_depth=10, min_samples_leaf=10, min_samples_split=10)\n",
    "\n",
    "time_start = time.time()\n",
    "DT_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(DT_model, open(\"Models/decision_tree.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = DT_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Decision Tree', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestClassifier(n_estimators=25, random_state=42, max_depth=10, min_samples_leaf=5, criterion='entropy', oob_score=True) \n",
    "\n",
    "time_start = time.time()\n",
    "RF_model.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(RF_model, open(\"Models/random_forest.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = RF_model.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['Random Forest', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 10, 'learning_rate': 0.1, 'objective': 'multi:softprob', 'num_class': 3, 'n_estimators' : 500}\n",
    "\n",
    "# Create the DMatrix from X_train and y_train_1\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_1)\n",
    "\n",
    "time_start = time.time()\n",
    "# Create the model and train\n",
    "xgb_model = xgb.train(params, dtrain)\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(xgb_model, open(\"Models/xgboost.pkl\", \"wb\"))\n",
    "\n",
    "# Create a DMatrix from the test data\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Get the class with the highest probability for each sample\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test_1, y_pred)\n",
    "precision = precision_score(y_test_1, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_1, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_1, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['XGBoost', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test_1, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (by F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model_f1 = SVC(kernel='poly', random_state=42, decision_function_shape='ovr', gamma='scale', coef0=5.287036, degree=4, max_iter=1000000)\n",
    "\n",
    "time_start = time.time()\n",
    "SVC_model_f1.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(SVC_model_f1, open(\"Models/svm_f1.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = SVC_model_f1.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['SVM_by_f1', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (by Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_model_time = SVC(kernel='poly', random_state=42, decision_function_shape='ovo', gamma='auto', coef0=8.523257, degree=2, max_iter=1000000)\n",
    "\n",
    "time_start = time.time()\n",
    "SVC_model_time.fit(X_train, y_train)\n",
    "time_end = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "pickle.dump(SVC_model_time, open(\"Models/svm_time.pkl\", \"wb\"))\n",
    "\n",
    "y_pred = SVC_model_time.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "total_time = time_end - time_start\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['SVM_by_time', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network (by F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Hyperparameters = {'batch_size': 16, 'learning_rate': 0.1, 'num_epochs': 300, 'optimizer': 'Adagrad'}\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "batch_size = Best_Hyperparameters['batch_size']\n",
    "learning_rate = Best_Hyperparameters['learning_rate']\n",
    "epochs = Best_Hyperparameters['num_epochs']\n",
    "optimizer_name = Best_Hyperparameters['optimizer']\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_1, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders with the current batch size\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "NN_model_f1 = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "if optimizer_name == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(NN_model_f1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = NN_model_f1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print the progress\n",
    "    if epoch % 10 == 9:\n",
    "        print('Epoch: {}/{}...'.format(epoch+1, epochs), 'Loss: {:.6f}'.format(running_loss/len(train_loader)))\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(NN_model_f1.state_dict(), 'Models/NN_f1.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "NN_model_f1.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = NN_model_f1(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "total_time = end_time-start_time\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['NN_by_f1', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network (by time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Hyperparameters = {'batch_size': 64, 'learning_rate': 0.0001, 'num_epochs': 50, 'optimizer': 'RMSprop'}\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "batch_size = Best_Hyperparameters['batch_size']\n",
    "learning_rate = Best_Hyperparameters['learning_rate']\n",
    "epochs = Best_Hyperparameters['num_epochs']\n",
    "optimizer_name = Best_Hyperparameters['optimizer']\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_1, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders with the current batch size\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "NN_model_time = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "if optimizer_name == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(NN_model_time.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = NN_model_time(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(NN_model_time.state_dict(), 'Models/NN_time.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "NN_model_time.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = NN_model_time(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "total_time = end_time-start_time\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['NN_by_time', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (by F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_Hyperparameters = {'batch_size': 32, 'learning_rate': 0.001, 'num_epochs': 100, 'optimizer': 'Adam'}\n",
    "\n",
    "# Get hyperparameters\n",
    "batch_size = Best_Hyperparameters['batch_size']\n",
    "learning_rate = Best_Hyperparameters['learning_rate']\n",
    "num_epochs = Best_Hyperparameters['num_epochs']\n",
    "optimizer_name = Best_Hyperparameters['optimizer']\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "\n",
    "        super(Classifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=20, kernel_size=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride = 2)\n",
    "\n",
    "        # Second layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=20, out_channels=40, kernel_size=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride = 2)\n",
    "\n",
    "        # Third layer (fully connected)\n",
    "        self.fc1 = nn.Linear(in_features=40, out_features=20)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Softmax Classifier\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    # specifying the flow of data through the layers\n",
    "    def forward(self, x):\n",
    "        # Pass through conv1\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        # Pass through conv2\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # Flatten the output before fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        output = self.softmax(x)\n",
    "\n",
    "        return output  # Return the output\n",
    "    \n",
    "\n",
    "# Assuming X_train and X_test have shapes (num_samples, num_features)\n",
    "X_train_tensor = torch.tensor(X_train.transpose(0, 1), dtype=torch.float32).unsqueeze(1)  # Shape: (num_features, num_samples) -> (num_samples, 1, num_features)\n",
    "X_test_tensor = torch.tensor(X_test.transpose(0, 1), dtype=torch.float32).unsqueeze(1)    # Shape: (num_features, num_samples) -> (num_samples, 1, num_features)\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train_1, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_1, dtype=torch.long)\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  # amount of data to be loaded each time\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the more complex CNN\n",
    "CNN_f1 = Classifier(0.67)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if optimizer_name == 'Adam':\n",
    "    optimizer = optim.Adam(CNN_f1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = CNN_f1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the model to a file\n",
    "torch.save(CNN_f1.state_dict(), 'Models/NN_time.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "CNN_f1.eval()\n",
    "with torch.no_grad():\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = CNN_f1(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)   # get the index of the max log-probability\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "total_time = end_time-start_time\n",
    "\n",
    "# Add the results to the list\n",
    "results.append(['CNN', accuracy, precision, recall, f1, total_time])\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "labels = ['GALAXY (1)', 'QSO (2)', 'STAR (3)']\n",
    "\n",
    "# Create a heatmap with the labels\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (by time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi ^_^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Total Time'])\n",
    "results_df.to_csv('Model_Training_Results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
